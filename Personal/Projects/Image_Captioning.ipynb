{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import string\n",
    "from PIL import Image\n",
    "from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from random import shuffle\n",
    "from keras.layers import LSTM, Embedding, Dense, Dropout, Input\n",
    "from keras.layers.merge import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/image_captioning/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_text(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = text.split()\n",
    "    text = [word.lower() for word in text]\n",
    "    text = [w.translate(table) for w in text]\n",
    "    text = [word for word in text if len(word) > 1]\n",
    "    text = [word for word in text if word.isalpha()]\n",
    "    text = 'startseq ' + ' '.join(text) + ' endseq'\n",
    "    return text\n",
    "\n",
    "def load_image(filename):\n",
    "    path = DATA_DIR + 'images/'\n",
    "    img = Image.open(path + filename)\n",
    "    img = img.resize((224, 224))\n",
    "    img_arr = np.array(img)\n",
    "    img_arr = np.expand_dims(img_arr, axis=0)\n",
    "    img_arr = preprocess_input(img_arr)\n",
    "    return img_arr\n",
    "\n",
    "def extract_features(feature_extractor_model, filename):\n",
    "    img_arr = load_image(filename) \n",
    "    feature_vector = feature_extractor_model.predict(img_arr) \n",
    "    feature_vector = np.reshape(feature_vector, (-1, ))\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_dataframe = pd.read_csv(DATA_DIR + 'results.csv', delimiter='|').drop('comment_number', axis=1)\n",
    "captions_dataframe['comment'] = captions_dataframe['comment'].apply(fix_text)\n",
    "captions_dataframe['image_name'] = captions_dataframe['image_name'].apply(lambda x: x.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_dataframe.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_caption_length = captions_dataframe['comment'].apply(lambda x: len(x.split())).max()\n",
    "\n",
    "word_count_threshold = 10\n",
    "word_counts = {}\n",
    "\n",
    "for sentence in captions_dataframe['comment']:\n",
    "    for w in sentence.split():\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "vocab_size = len(vocab) + 1\n",
    "\n",
    "print('Preprocessed words {} -> {}'.format(len(word_counts), len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "index_to_word = []\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index\n",
    "    index_to_word.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_list_by_name_dict = {}\n",
    "\n",
    "for _, (name, caption) in captions_dataframe.iterrows():\n",
    "    if name in captions_list_by_name_dict: captions_list_by_name_dict[name].append(caption)\n",
    "    else: captions_list_by_name_dict[name] = [caption]\n",
    "\n",
    "grouped_captions = []\n",
    "\n",
    "for key, value in captions_list_by_name_dict.items():\n",
    "    grouped_captions.append([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(feature_extractor_model, grouped_captions):\n",
    "    image_features, in_sequence, out_sequennce = [], [], []\n",
    "    while True:\n",
    "        shuffle(grouped_captions)\n",
    "\n",
    "        for name, captions in grouped_captions:\n",
    "            im_fea = extract_features(feature_extractor_model, name + '.jpg')\n",
    "        \n",
    "            for caption in captions:\n",
    "                encoded_sequence = [word_to_index[word] for word in caption.split() if word in word_to_index]\n",
    "                for i in range(1, len(encoded_sequence)):\n",
    "                    in_seq, out_seq = encoded_sequence[:i], encoded_sequence[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_caption_length)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "                    image_features.append(im_fea)\n",
    "                    in_sequence.append(in_seq)\n",
    "                    out_sequennce.append(out_seq)\n",
    "                \n",
    "            \n",
    "            yield [[np.array(image_features), np.array(in_sequence)], np.array(out_sequennce)]\n",
    "            image_features, in_sequence, out_sequennce = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature Extractor Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_model = VGG19(weights='imagenet')\n",
    "feature_extractor_model = Model(feature_extractor_model.input, feature_extractor_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(grouped_captions)\n",
    "train_val_grouped_captions, test_grouped_captions = train_test_split(grouped_captions, test_size=0.2, random_state=42)\n",
    "train_grouped_captions, val_grouped_captions = train_test_split(train_val_grouped_captions, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = data_generator(feature_extractor_model, train_grouped_captions)\n",
    "val_generator = data_generator(feature_extractor_model, val_grouped_captions)\n",
    "test_generator = data_generator(feature_extractor_model, test_grouped_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in train_generator:\n",
    "    print(item[0][0].shape, item[0][1].shape, item[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><a href=\"https://www.geeksforgeeks.org/pre-trained-word-embedding-using-glove-in-nlp-models/\">GloVe</a></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir = DATA_DIR + 'glove.6B.200d.txt'\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(glove_dir) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=np.float32)\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_dim = 200\n",
    "\n",
    "# Get 200-dim dense vector for each of the words in out vocabulary\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in word_to_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Caption Generator Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_input = Input(shape=(4096,))\n",
    "feature_layer1 = Dropout(0.5)(feature_input)\n",
    "feature_layer2 = Dense(256, activation='relu')(feature_layer1)\n",
    "caption_input = Input(shape=(max_caption_length, ))\n",
    "caption_layer1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(caption_input)\n",
    "caption_layer2 = Dropout(0.5)(caption_layer1)\n",
    "caption_layer3 = LSTM(256)(caption_layer2)\n",
    "decoder1 = add([feature_layer2, caption_layer3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "caption_generation_model = Model(inputs=[feature_input, caption_input], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_generation_model.layers[2].set_weights([embedding_matrix])\n",
    "caption_generation_model.layers[2].trainable = False\n",
    "\n",
    "caption_generation_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "epochs = 20\n",
    "batch_size = 3\n",
    "steps = len(grouped_captions) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = caption_generation_model.fit(\n",
    "#     train_generator, \n",
    "#     epochs=epochs, \n",
    "#     steps_per_epoch=steps, \n",
    "#     validation_data=val_generator, \n",
    "#     validation_steps=steps,\n",
    "#     batch_size=batch_size,\n",
    "#     verbose=1\n",
    "# )\n",
    "# caption_generation_model.save(DATA_DIR + 'saved_model/model_' + str(20) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_loss = history.history['loss']\n",
    "# epoch_count = range(1, len(training_loss) + 1)\n",
    "# plt.figure() \n",
    " \n",
    "# plt.plot(epoch_count, training_loss, 'r--')\n",
    "# plt.legend(['Training Loss'])\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('First 20 Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caption_generation_model.optimizer.lr = 0.0001\n",
    "# epochs = 10\n",
    "# batch_size = 6\n",
    "# steps = len(grouped_captions) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history1 = caption_generation_model.fit(\n",
    "#     train_generator, \n",
    "#     epochs=epochs, \n",
    "#     steps_per_epoch=steps, \n",
    "#     validation_data=val_generator, \n",
    "#     validation_steps=steps,\n",
    "#     batch_size=batch_size,\n",
    "#     verbose=1\n",
    "# )\n",
    "# caption_generation_model.save(DATA_DIR + 'saved_model/model_' + str(30) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_loss = history1.history['loss']\n",
    "# epoch_count = range(1, len(training_loss) + 1)\n",
    "# plt.figure()  \n",
    "\n",
    "# plt.plot(epoch_count, training_loss, 'r--')\n",
    "# plt.legend(['Training Loss'])\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Last 10 Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_features):\n",
    "    in_text = 'startseq'\n",
    "    for _ in range(max_caption_length):\n",
    "        sequence = [word_to_index[w] for w in in_text.split() if w in word_to_index]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_caption_length)\n",
    "        yhat = caption_generation_model.predict([image_features, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = index_to_word[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final.capitalize() + '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, caption_list in test_grouped_captions[:3]:\n",
    "    image_features = extract_features(feature_extractor_model, name + '.jpg').reshape(1, 4096)\n",
    "    caption = generate_caption(image_features)\n",
    "    with Image.open(DATA_DIR + 'images/' + name + '.jpg') as img:\n",
    "        img = np.array(img)\n",
    "        img = img / 255.0\n",
    "        print(caption)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19217652c784c0eb9511f8f5b096e4f8c7203998629d570e7b12322847e1aa51"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
